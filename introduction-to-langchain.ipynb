{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonybaloney/introduction-to-langchain-workshop/blob/main/introduction-to-langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_cq1IsWCawQ"
      },
      "source": [
        "# Introduction to Langchain\n",
        "\n",
        "LangChain is a framework for developing applications powered by language models\n",
        "\n",
        "- GitHub: https://github.com/langchain-ai/langchain\n",
        "- Docs: https://python.langchain.com/docs/get_started\n",
        "\n",
        "## Outlines\n",
        "1. Main components -- Model, Prompt Template, Output Parser\n",
        "2. Chains\n",
        "3. Memory\n",
        "4. Retriever (RAG)\n",
        "5. Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eitV0BpiDiWE"
      },
      "source": [
        "## 0. Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "nDMgy3hjqOki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1a119f2-5e15-4f43-e744-cc72bf84296e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.0.349 in /usr/local/lib/python3.10/dist-packages (0.0.349)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.349) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.349) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.349) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.349) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.349) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.349) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.349) (0.0.1)\n",
            "Requirement already satisfied: langchain-core<0.1,>=0.0.13 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.349) (0.0.13)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.349) (0.0.70)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.349) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.349) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.349) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.349) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.349) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.349) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.349) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.349) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.349) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.349) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.349) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.349) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.1,>=0.0.13->langchain==0.0.349) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.1,>=0.0.13->langchain==0.0.349) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.349) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.349) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.349) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.349) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.349) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.349) (3.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.1,>=0.0.13->langchain==0.0.349) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.1,>=0.0.13->langchain==0.0.349) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.349) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.0.349"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOAGAoxEmZBy"
      },
      "source": [
        "## 1. Main Components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL4ROOEymiTc"
      },
      "source": [
        "### 1.1. Model\n",
        "\n",
        "\n",
        "https://python.langchain.com/docs/modules/model_io/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLRbJPBxEQLE"
      },
      "source": [
        "#### Model - Hugging Face (google/flan-t5-xxl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhxtCSyi7YFn",
        "outputId": "12ca6cc9-4d96-46c9-d26b-3136bf249d97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub==0.19.4 in /usr/local/lib/python3.10/dist-packages (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.19.4) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.19.4) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.19.4) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.19.4) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.19.4) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.19.4) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.19.4) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.19.4) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.19.4) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.19.4) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.19.4) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub==0.19.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "M-uUHYMR9-zo",
        "outputId": "6aeabf28-9f3d-4022-f391-23fdc02c5812"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-d621b66c16bf>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HUGGINGFACEHUB_API_TOKEN\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HUGGINGFACEHUB_API_TOKEN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrepo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"google/flan-t5-xxl\"\u001b[0m  \u001b[0;31m# See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     37\u001b[0m   )\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret HUGGINGFACEHUB_API_TOKEN does not exist."
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "repo_id = \"google/flan-t5-xxl\"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
        "\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNJdVmW90hM0"
      },
      "outputs": [],
      "source": [
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "print(llm(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhvK9K9-daNz"
      },
      "source": [
        "#### Model - Local (LlamaCPP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVesRLYVlG_-"
      },
      "outputs": [],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaSHd5PHq17E"
      },
      "outputs": [],
      "source": [
        "!wget https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9EoHngosUaF"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/content/mistral-7b-instruct-v0.1.Q4_0.gguf\",\n",
        "    n_gpu_layers=200,\n",
        "    n_ctx=32000,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjHKXtLdoGaY"
      },
      "outputs": [],
      "source": [
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "print(llm(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPAdlHTcEKWN"
      },
      "source": [
        "#### Model - OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "0k1ECxrM7H5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b34236e-2481-4b15-b8d2-2549b1a8ab52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==1.3.9 in /usr/local/lib/python3.10/dist-packages (1.3.9)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.9) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.3.9) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.9) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.9) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.3.9) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.9) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai==1.3.9) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.3.9) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.3.9) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.3.9) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.3.9) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.3.9) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==1.3.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "iSa6lOIoz2kC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get it from https://platform.openai.com/account/api-keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_BASE\"] = userdata.get('OPENAI_API_BASE')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### LLM Model\n",
        "\n",
        "LLMs in LangChain refer to pure text completion models. The APIs they wrap take a string prompt as input and output a string completion."
      ],
      "metadata": {
        "id": "JpFyBH7AeqLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import AzureOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "\n",
        "llm = AzureOpenAI(temperature=0, deployment_name=\"text-davinci-002\", model_name=\"davinci-002\", api_version=\"2023-05-15\")\n",
        "print(f\"Using LLM model: {llm.model_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxa0q1Nzep8m",
        "outputId": "d934a90c-751d-4da0-eaae-ab5ebad7a181"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using LLM model: davinci-002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/llms/openai.py:872: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://llm-workshop-pycon-swe.openai.azure.com/ to https://llm-workshop-pycon-swe.openai.azure.com/openai.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/llms/openai.py:879: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/llms/openai.py:887: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://llm-workshop-pycon-swe.openai.azure.com/ to https://llm-workshop-pycon-swe.openai.azure.com/openai.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QReKGGfgfHHJ",
        "outputId": "55f4b7df-d2f4-4aef-8081-430a212771c7"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I'm starting a company that makes colorful socks. I need a good company name. I'm thinking of something like \"Sock It To Me\" or \"Sock It Up\" or \"Sock It Up\" or \"Sock It Up\" or \"Sock It Up\" or \"Sock It Up\" or \"Sock It Up\" or \"Sock It Up\" or \"Sock It Up\" or \"Sock It Up\" or \"Sock It Up\" or \"Sock It Up\" or \"Sock It Up\" or \"\n",
            "\n",
            "2017-07-20 17:00:00\n",
            "\n",
            "2017-07-20 17:00:00\n",
            "\n",
            "2017-07-20 17:00:00\n",
            "\n",
            "2017-07-20 17:00:00\n",
            "\n",
            "2017-07-20 17:00:00\n",
            "\n",
            "2017-07-20 17:00:00\n",
            "\n",
            "2017-07-20 17:00:00\n",
            "\n",
            "2017-07-20 17:00:00\n",
            "\n",
            "2017-07-20 17:00:00\n",
            "\n",
            "2017-07-20 17:00:00\n",
            "\n",
            "2017-07-20 17:00:00\n",
            "\n",
            "2017-07-20 17:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Chat Model\n",
        "\n",
        "Chat models are often backed by LLMs but tuned specifically for having conversations. And, crucially, their provider APIs use a different interface than pure text completion models. Instead of a single string, they take a list of chat messages as input. Usually these messages are labeled with the speaker (usually one of \"System\", \"AI\", and \"Human\"). And they return an AI chat message as output"
      ],
      "metadata": {
        "id": "NgAXDq0QfBva"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "ao51koO2FduX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2131265c-5f90-442c-d18c-7df09858c5fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using chat model: gpt-3.5-turbo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/chat_models/azure_openai.py:161: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://llm-workshop-pycon-swe.openai.azure.com/ to https://llm-workshop-pycon-swe.openai.azure.com/openai.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/chat_models/azure_openai.py:168: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/chat_models/azure_openai.py:176: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://llm-workshop-pycon-swe.openai.azure.com/ to https://llm-workshop-pycon-swe.openai.azure.com/openai.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain.schema import HumanMessage\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "os.environ[\"OPENAI_API_VERSION\"] = userdata.get(\"OPENAI_API_VERSION\")\n",
        "chat = AzureChatOpenAI(azure_deployment=\"gpt-4\")\n",
        "print(f\"Using chat model: {chat.model_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"What would be a good company name for a company that makes colorful socks?\")]\n",
        "\n",
        "chat(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIvIhw7Efy3I",
        "outputId": "b5e2bcfa-ea34-4f0d-de4f-e0be09c486fd"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='1. \"Vibrant Steps\"\\n2. \"Kaleidoscope Socks\"\\n3. \"Rainbow Toes\"\\n4. \"BoldSox\"\\n5. \"Spectrum Strides\"\\n6. \"Colorful Comforts\"\\n7. \"Chroma Crew\"\\n8. \"Sunny Socks Co.\"\\n9. \"ColorPop Hosiery\"\\n10. \"Prism Peds\"\\n11. \"Technicolor Treads\"\\n12. \"Saturated Steps\"\\n13. \"Feet Fiestas\"\\n14. \"Artistic Ankles\"\\n15. \"Dazzling Digits\"\\n16. \"Socks Spectrum\"\\n17. \"Rainbow Wraps\".\\n18. \"Hue Huggers\"\\n19. \"Bursting Bright Socks\"\\n20. \"Vivid Velvet Socks\".')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjf7UO-ved4N"
      },
      "source": [
        "#### Bonus: Using LLM as a question-answering model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "l_W_MxsMeh1b"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Question: What would be a good company name for a company that makes colorful socks?\n",
        "\n",
        "Let's think step by step.\n",
        "\n",
        "Answer: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rie6cTv9ex9L",
        "outputId": "1c21f723-46d9-4707-dbbc-94ce6120a062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. What is the name of the company?\n",
            "\n",
            "2. What is the name of the company?\n",
            "\n",
            "3. What is the name of the company?\n",
            "\n",
            "4. What is the name of the company?\n",
            "\n",
            "5. What is the name of the company?\n",
            "\n",
            "6. What is the name of the company?\n",
            "\n",
            "7. What is the name of the company?\n",
            "\n",
            "8. What is the name of the company?\n",
            "\n",
            "9. What is the name of the company?\n",
            "\n",
            "10. What is the name of the company?\n",
            "\n",
            "11. What is the name of the company?\n",
            "\n",
            "12. What is the name of the company?\n",
            "\n",
            "13. What is the name of the company?\n",
            "\n",
            "14. What is the name of the company?\n",
            "\n",
            "15. What is the name of the company?\n",
            "\n",
            "16. What is the name of the company?\n",
            "\n",
            "17. What is the name of the company?\n",
            "\n",
            "18. What is the name of the company?\n",
            "\n",
            "19. What is the name of the company?\n",
            "\n",
            "20. What is the name of the company?\n",
            "\n",
            "21. What is the name of the company?\n",
            "\n",
            "22. What is the name of the company?\n",
            "\n",
            "23. What is the name of the company?\n",
            "\n",
            "24. What is the name of the company?\n",
            "\n",
            "25. What is the name of the company?\n",
            "\n",
            "26. What is the name\n"
          ]
        }
      ],
      "source": [
        "print(llm(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShnjkqhBIXrZ"
      },
      "source": [
        "### 1.2. Prompt Templates\n",
        "\n",
        "Prompt templates are pre-defined recipes for generating prompts for language models.\n",
        "\n",
        "https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "N9HmnUcAEGyy"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Let's think step by step, and then summarize the final answer in this format:\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4OBbb1iS8aN",
        "outputId": "39900744-665b-4b11-a9aa-bdde09a7effa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is a good name for a company that makes video games\n",
            "\n",
            "Let's think step by step, and then summarize the final answer in this format:\n",
            "\n",
            "Answer: \n"
          ]
        }
      ],
      "source": [
        "prompt_text = prompt.format(question=\"What is a good name for a company that makes video games\")\n",
        "print(prompt_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21brmLZKVn35",
        "outputId": "8d72a0e9-5ebc-4469-d8e4-28e71e88353c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. The name of the company should be short and easy to remember. 2. The name should be easy to pronounce. 3. The name should be easy to spell. 4. The name should be easy to pronounce. 5. The name should be easy to spell. 6. The name should be easy to pronounce. 7. The name should be easy to spell. 8. The name should be easy to pronounce. 9. The name should be easy to spell. 10. The name should be easy to pronounce. 11. The name should be easy to spell. 12. The name should be easy to pronounce. 13. The name should be easy to spell. 14. The name should be easy to pronounce. 15. The name should be easy to spell. 16. The name should be easy to pronounce. 17. The name should be easy to spell. 18. The name should be easy to pronounce. 19. The name should be easy to spell. 20. The name should be easy to pronounce. 21. The name should be easy to spell. 22. The name should be easy to pronounce. 23. The name should be easy to spell\n"
          ]
        }
      ],
      "source": [
        "print(llm(prompt_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaorfPX0govN"
      },
      "source": [
        "### 1.3. Output Parser\n",
        "\n",
        "Language models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "U8LCQLcpjhBL"
      },
      "outputs": [],
      "source": [
        "customer_review = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings:\\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "qLVXesssbLLK"
      },
      "outputs": [],
      "source": [
        "review_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product \\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "\n",
        "text: {text}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Ca6mcIYThGob"
      },
      "outputs": [],
      "source": [
        "prompt_template = PromptTemplate(template=review_template, input_variables=[\"text\"],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7_8rkGmhQqL",
        "outputId": "79dd3ac7-1ee0-4a58-cc8f-dabf84acb3c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: I bought this leaf blower for my wife for her birthday. It arrived in two days, and she was so happy with it that she was speechless. I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
            "\n",
            "text: I bought this leaf blower for my wife for her birthday. It arrived in two days, and she was so happy with it that she was speechless. I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
            "\n",
            "text: I bought this leaf blower for my wife for her birthday. It arrived in two days, and she was so happy with it that she was speechless. I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
            "\n",
            "text: I bought this leaf blower for my wife for her birthday. It arrived in two days, and she was so happy with it\n"
          ]
        }
      ],
      "source": [
        "answer = llm(prompt_template.format(text=customer_review))\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Throw an error because answer is just a string - not dict\n",
        "answer['gift']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "THgnjFMchLKA",
        "outputId": "9f11fc6a-fea1-4fff-bc92-66920bec9832"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-706a2f6a8b4e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Throw an error because answer is just a string - not dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gift'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU_KoI4ClNVs"
      },
      "source": [
        "#### Using output parsers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "CpnFixOXhZrs"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "JLIwj1odhelF"
      },
      "outputs": [],
      "source": [
        "gift_schema = ResponseSchema(name=\"gift\",\n",
        "                             description=\"Was the item purchased\\\n",
        "                             as a gift for someone else? \\\n",
        "                             Answer True if yes,\\\n",
        "                             False if not or unknown.\")\n",
        "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
        "                                      description=\"How many days\\\n",
        "                                      did it take for the product\\\n",
        "                                      to arrive? If this \\\n",
        "                                      information is not found,\\\n",
        "                                      output -1.\")\n",
        "price_value_schema = ResponseSchema(name=\"price_value\",\n",
        "                                    description=\"Extract any\\\n",
        "                                    sentences about the value or \\\n",
        "                                    price, and output them as a \\\n",
        "                                    comma separated Python list.\")\n",
        "\n",
        "response_schemas = [gift_schema,\n",
        "                    delivery_days_schema,\n",
        "                    price_value_schema]\n",
        "\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAR5ZLxOhjHR",
        "outputId": "b2acf810-8813-44d9-f32a-0e1cbd594639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
            "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
            "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "0kIknSfUjuhM"
      },
      "outputs": [],
      "source": [
        "review_template_with_instructions = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "text: {text}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "FCwBl5tThnYa"
      },
      "outputs": [],
      "source": [
        "prompt_template_with_output = PromptTemplate(template=review_template_with_instructions, input_variables=[\"text\"], partial_variables={\"format_instructions\": format_instructions})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "QDkxfdpgh6dl"
      },
      "outputs": [],
      "source": [
        "prompt_and_model = prompt_template_with_output | llm\n",
        "output = prompt_and_model.invoke({\"text\": customer_review})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "3Rgm9bZoiXp9",
        "outputId": "bd66571b-2d77-417d-b1d5-72dca56d50ca"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutputParserException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/json.py\u001b[0m in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mjson_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_json_markdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/json.py\u001b[0m in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;31m# Parse the JSON string into a Python dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-a92d5a2dc698>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# -- which is good because we want to know early that the model cannot achieve the tasks we want it to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    177\u001b[0m             )\n\u001b[1;32m    178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             return self._call_with_config(\n\u001b[0m\u001b[1;32m    180\u001b[0m                 \u001b[0;32mlambda\u001b[0m \u001b[0minner_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m    884\u001b[0m         )\n\u001b[1;32m    885\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m             output = call_func_with_variable_args(\n\u001b[0m\u001b[1;32m    887\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/base.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             return self._call_with_config(\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0;32mlambda\u001b[0m \u001b[0minner_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/base.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mStructured\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/structured.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mexpected_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_schemas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparse_and_check_json_markdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/json.py\u001b[0m in \u001b[0;36mparse_and_check_json_markdown\u001b[0;34m(text, expected_keys)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mjson_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_json_markdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOutputParserException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got invalid JSON object. Error: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexpected_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutputParserException\u001b[0m: Got invalid JSON object. Error: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ],
      "source": [
        "# Note: Sometimes, if the model cannot extract the information, this can throw an error\n",
        "# -- which is good because we want to know early that the model cannot achieve the tasks we want it to\n",
        "\n",
        "result = output_parser.invoke(output)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CRSY6K-ia8N"
      },
      "outputs": [],
      "source": [
        "result[\"gift\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzPkq73xlFuJ"
      },
      "source": [
        "### Bonus: Chaining Stuffs\n",
        "\n",
        "Because all of the objects implements the `Runnable` interface. It can be chained together.\n",
        "\n",
        "More info: https://python.langchain.com/docs/expression_language/why"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXisUIdvkvrI"
      },
      "outputs": [],
      "source": [
        "output_chain = prompt_template_with_output | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JILirx7YlfNZ"
      },
      "outputs": [],
      "source": [
        "output_chain.invoke({\"text\": customer_review})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RLhONceoZWb"
      },
      "source": [
        "## 2. Chains\n",
        "\n",
        "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components.\n",
        "\n",
        "LangChain provides two high-level frameworks for chaining components. The legacy approach is to use the Chain interface. The updated approach is to use the LangChain Expression Language (LCEL).\n",
        "\n",
        "https://python.langchain.com/docs/modules/chains/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5caIbjjssPw"
      },
      "source": [
        "### Old way\n",
        "\n",
        "The legacy interface for chained applications. We define a Chain very generically as a sequence of calls to components, which can include other chains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPOyda-nopOm"
      },
      "outputs": [],
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt_template_with_output, llm=llm, output_parser=output_parser)\n",
        "\n",
        "llm_chain.run(text=customer_review)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9mEHvGluJD1"
      },
      "source": [
        "### New way\n",
        "\n",
        "LCEL provides an intuitive and readable syntax for composition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0lIgnwit76G"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "chain = (\n",
        " { \"text\": RunnablePassthrough() }\n",
        " | prompt_template_with_output\n",
        " | llm\n",
        " | output_parser\n",
        ")\n",
        "chain.invoke(customer_review)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJo30qgXwiAj"
      },
      "source": [
        "## 3. Memory\n",
        "\n",
        "Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation\n",
        "\n",
        "https://python.langchain.com/docs/modules/memory/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opw5Oym9yNrj"
      },
      "source": [
        "### Manipulating the memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "M9dy8mE-xUGA"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "memory.chat_memory.add_user_message(\"hi!\")\n",
        "memory.chat_memory.add_ai_message(\"what's up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DkxLtF1xj-a",
        "outputId": "41516ba2-9d59-4258-dfba-ae429157fb8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: hi!\n",
            "AI: what's up?\n"
          ]
        }
      ],
      "source": [
        "print(memory.load_memory_variables({})['history'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "NOQ9hIRrxyWn"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"how yo doin'\"}, {\"output\": \"fine. thank you!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFSvqBWrx12V",
        "outputId": "f58a1f11-b6f1-459b-f3af-8c6df62645d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: hi!\n",
            "AI: what's up?\n",
            "Human: how yo doin'\n",
            "AI: fine. thank you!\n"
          ]
        }
      ],
      "source": [
        "print(memory.load_memory_variables({})['history'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
        "\n",
        "New human question: {question}\n",
        "Response:\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "HzGcpLAIS6-t"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XusFs3R6OKfs"
      },
      "source": [
        "#### Without memory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "no_memory_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=False,\n",
        ")"
      ],
      "metadata": {
        "id": "0tphRLEnSoqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_memory_chain({\"question\": \"Hello, My name is Junior\"})"
      ],
      "metadata": {
        "id": "_Yrd3V7RSshG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_memory_chain({\"question\": \"I have just introduced myself. What is my name?\"})"
      ],
      "metadata": {
        "id": "ICoO_RhwTbP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB60Gvopz93e"
      },
      "source": [
        "#### With memory - LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya6BiTlNwj70"
      },
      "outputs": [],
      "source": [
        "# Notice that \"chat_history\" is present in the prompt template\n",
        "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "New human question: {question}\n",
        "Response:\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "# Notice that we need to align the `memory_key`\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "with_memory_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=False,\n",
        "    memory=memory ## here - we are giving it a memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzznWx-iyHKX"
      },
      "outputs": [],
      "source": [
        "with_memory_chain({\"question\": \"ay yo!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Za-ap-OiytQ5"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"how yo doin' My name is Junior. Nice to meet you.\"}, {\"output\": \"Nice to meet you, Junior!. I am fine. Thank you!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYJx4hSVzd2L"
      },
      "outputs": [],
      "source": [
        "# Some model, like Hugging Face's, might not always work\n",
        "\n",
        "with_memory_chain({\"question\": \"what is my name again?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3yTC9jc0AnV"
      },
      "source": [
        "#### With memory - ConversationChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a-_9tWUzgCH"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaG9mlIH0OzQ"
      },
      "outputs": [],
      "source": [
        "print(conversation.predict(input=\"how yo doin' My name is Junior. Nice to meet you.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZ5xpupL05hb"
      },
      "outputs": [],
      "source": [
        "print(conversation.predict(input=\"what is my name again?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoJl3gq51Iaf"
      },
      "source": [
        "### Bonus: Different types of Memory\n",
        "\n",
        "More info: https://python.langchain.com/docs/modules/memory/types/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZvHCuES1Btw"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=ConversationSummaryMemory(llm=llm) # Note that the ConversationSummaryMemory will need an LLM as an input to do the summarization\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZpyqBUF1loI"
      },
      "outputs": [],
      "source": [
        "# Notice the prompt that ConversationChain format for us\n",
        "\n",
        "conversation.predict(input=\"how yo doin' My name is Junior. Nice to meet you.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjhU1qo01oxG"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"I like the color red. My favorite subject is Math.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXvbZiVa1sYe"
      },
      "outputs": [],
      "source": [
        "conversation.predict(input=\"Can you guess my favorite subject?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fJb7z_02Up4"
      },
      "source": [
        "## 4. Retriever (RAG)\n",
        "\n",
        "Many LLM applications require user-specific data that is not part of the model's training set. The primary way of accomplishing this is through Retrieval Augmented Generation (RAG)\n",
        "\n",
        "More info: https://python.langchain.com/docs/modules/data_connection/\n",
        "\n",
        "***Note: LlamaCPP model users should skip this, as it might crash the notebook due to limited resource***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt\"\n",
        "res = requests.get(url)\n",
        "with open(\"state_of_the_union.txt\", \"w\") as f:\n",
        "  f.write(res.text)"
      ],
      "metadata": {
        "id": "9mTOI-k_U5_K"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document Loader\n",
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader('./state_of_the_union.txt')\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "V1RIf3JRXpmY"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Splitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "uFdgSU33XxCm"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hugging Face Embeddings"
      ],
      "metadata": {
        "id": "JghPkv4NW6CZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers==2.2.2 faiss-cpu==1.7.4"
      ],
      "metadata": {
        "id": "kYpXyYZTXCgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "MFjozFrxWMTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "db = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "UbQr4wpaYDaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "docs = db.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "OziBKHodYHuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LlamaCPP Embeddings\n",
        "\n",
        "***Note: LlamaCPP model users should skip this, as it might crash the notebook due to limited resource***"
      ],
      "metadata": {
        "id": "dj3kR-n4BJi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb==0.4.19 tiktoken==0.5.2"
      ],
      "metadata": {
        "id": "7GLoWzZZB9dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import LlamaCppEmbeddings\n",
        "\n",
        "embeddings = LlamaCppEmbeddings(model_path=\"/content/mistral-7b-instruct-v0.1.Q4_0.gguf\")"
      ],
      "metadata": {
        "id": "V8AQ2flcBMrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "db = Chroma.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "G5ZU1HrQBgjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "docs = db.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "07Ksg9u_Bydd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OpenAI Embeddings"
      ],
      "metadata": {
        "id": "d1QyqtFAW9n7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers==2.2.2 faiss-cpu==1.7.4 tiktoken==0.5.2"
      ],
      "metadata": {
        "id": "BZ6AcBwlXO6j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb46561-3226-41e9-d53a-28982ed5fb0c"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers==2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faiss-cpu==1.7.4\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken==0.5.2\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers==2.2.2)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (0.19.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.2) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.2) (2.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (2023.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.5.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.5.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.5.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.5.2) (2023.11.17)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (2.1.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers==2.2.2) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers==2.2.2) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers==2.2.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers==2.2.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers==2.2.2) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers==2.2.2) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers==2.2.2) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers==2.2.2) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=1a48f9015c116b1d8a761955b7d2b1245204359e1520636e6eff2b4503b85429\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: sentencepiece, faiss-cpu, tiktoken, sentence_transformers\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed faiss-cpu-1.7.4 sentence_transformers-2.2.2 sentencepiece-0.1.99 tiktoken-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "Bkshtx4tXWSj"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8X3UdtZT3Eu"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "db = FAISS.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "docs = db.similarity_search(query)\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "hXB0gcTXZhyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using Retriever\n",
        "\n",
        "LangChain supports many different retrieval algorithms and is one of the places where it adds the most value"
      ],
      "metadata": {
        "id": "RLmUHmcmX4bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "Qyn0OB_8ZFnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mClNGty3cPSU"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Let's think step by step.\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2ZysBXlc2zP"
      },
      "outputs": [],
      "source": [
        "answer = chain.invoke(\"Who is Ketanji Brown Jackson?\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "\n",
        "langchain.debug = True\n",
        "chain.invoke(\"What does the speech say about Russia?\")\n",
        "langchain.debug = False"
      ],
      "metadata": {
        "id": "RLAe4LMMno_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHRKT469h7Bq"
      },
      "source": [
        "## 5. Evaluation\n",
        "\n",
        "Building applications with language models involves many moving parts. One of the most critical components is ensuring that the outcomes produced by your models are reliable and useful across a broad array of inputs, and that they work well with your application's other software components.\n",
        "\n",
        "More info: https://python.langchain.com/docs/guides/evaluation/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHwM43IToaCY"
      },
      "source": [
        "### Generating test datasets & evaluate its accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain[docarray]==0.0.349 openai==1.3.9 huggingface_hub==0.19.4"
      ],
      "metadata": {
        "id": "1T_hTi1ke9Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvTqau3jloYq"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/Ryota-Kawamura/LangChain-for-LLM-Application-Development/main/OutdoorClothingCatalog_1000.csv\"\n",
        "res = requests.get(url)\n",
        "with open(\"OutdoorClothingCatalog_1000.csv\", \"w\") as f:\n",
        "  f.write(res.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "hugging_face_repo_id = \"google/flan-t5-xxl\"  # See https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads for some other options\n",
        "\n",
        "\n",
        "openai_llm = OpenAI(temperature=0)\n",
        "hf_llm = HuggingFaceHub(\n",
        "    repo_id=hugging_face_repo_id, model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
        ")\n",
        "chat_model = ChatOpenAI()"
      ],
      "metadata": {
        "id": "dyjx4jauhXrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "file = \"./OutdoorClothingCatalog_1000.csv\"\n",
        "loader = CSVLoader(file_path=file)\n",
        "data = loader.load()\n",
        "\n",
        "index = VectorstoreIndexCreator(\n",
        "    vectorstore_cls=DocArrayInMemorySearch\n",
        ").from_loaders([loader])"
      ],
      "metadata": {
        "id": "Si1tJCVteEQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[10].page_content)"
      ],
      "metadata": {
        "id": "PoxCSYgVe3dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_examples = [\n",
        "    {\n",
        "        \"query\": \"Do the Cozy Comfort Pullover Set\\\n",
        "        have side pockets?\",\n",
        "        \"answer\": \"Yes\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# But how can we automate the generation of these questions & answers ? (:thinking:)"
      ],
      "metadata": {
        "id": "7dV3rGhVfxB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.evaluation.qa import QAGenerateChain\n",
        "\n",
        "example_gen_chain = QAGenerateChain.from_llm(chat_model)"
      ],
      "metadata": {
        "id": "tEp0xsCtf0Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_examples = example_gen_chain.apply_and_parse(\n",
        "    [{\"doc\": t} for t in data[:5]]\n",
        ")"
      ],
      "metadata": {
        "id": "Af_JA1WfhhWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_examples[0]"
      ],
      "metadata": {
        "id": "-jZ8AKK4h6wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[0].page_content)"
      ],
      "metadata": {
        "id": "nIQqoKuQh8-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model under test\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=hf_llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=index.vectorstore.as_retriever(),\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "8_CIzYLOlMRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa.run(test_data_examples[0][\"query\"])"
      ],
      "metadata": {
        "id": "bqA3wlITiSg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = []\n",
        "for example in generated_examples:\n",
        "  test_data.append(example['qa_pairs'])\n",
        "\n",
        "test_data"
      ],
      "metadata": {
        "id": "hcHEKKW3l0dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = qa.apply(test_data)\n",
        "predictions"
      ],
      "metadata": {
        "id": "Ej5kBYpYiyfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`answer` is the actual answer expected from the question\n",
        "\n",
        "`result` is what model under test predicts\n",
        "\n",
        "Notice that the `answer` and the `result` is not an exact 1-to-1 match, BUT the content could be saying the same thing.\n",
        "\n",
        "This is why we need **ANOTHER** LLM model to help evaluate whether the answer and the predicted result is saying the same thing."
      ],
      "metadata": {
        "id": "Gy1i0dv8ukr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.evaluation.qa import QAEvalChain\n",
        "\n",
        "eval_chain = QAEvalChain.from_llm(openai_llm)"
      ],
      "metadata": {
        "id": "uN6aBWrdi0Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graded_outputs = eval_chain.evaluate(test_data, predictions)"
      ],
      "metadata": {
        "id": "3gSfsPaGmr1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, eg in enumerate(generated_examples):\n",
        "    print(f\"Example {i}:\")\n",
        "    print(\"Question: \" + predictions[i]['query'])\n",
        "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
        "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
        "    print('Verdict: ' + graded_outputs[i]['results'])\n",
        "    print()"
      ],
      "metadata": {
        "id": "14r0SlAJmxxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus: Evaluator"
      ],
      "metadata": {
        "id": "pEVhSc0knNhD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3JHYSlWls-T"
      },
      "outputs": [],
      "source": [
        "from langchain.evaluation import load_evaluator\n",
        "from langchain.evaluation import EvaluatorType\n",
        "\n",
        "evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=\"conciseness\")\n",
        "\n",
        "eval_result = evaluator.evaluate_strings(\n",
        "    prediction=\"What's 2+2? That's an elementary question. The answer you're looking for is that two and two is four.\",\n",
        "    input=\"What's 2+2?\",\n",
        ")\n",
        "\n",
        "print(f'Evaluation value (Y/N): {eval_result[\"value\"]}')\n",
        "print(f'Evaluation score: {eval_result[\"score\"]}')\n",
        "print(f'Evaluation reasoning: {eval_result[\"reasoning\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgEmosCPQHXO"
      },
      "outputs": [],
      "source": [
        "eval_result = evaluator.evaluate_strings(\n",
        "    prediction=\"four.\",\n",
        "    input=\"What's 2+2?\",\n",
        ")\n",
        "\n",
        "print(f'Evaluation value (Y/N): {eval_result[\"value\"]}')\n",
        "print(f'Evaluation score: {eval_result[\"score\"]}')\n",
        "print(f'Evaluation reasoning: {eval_result[\"reasoning\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mixWcScRCc9"
      },
      "source": [
        "#### Different type of evaluator -- Labeled Criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2j_Cx1UQmNf_"
      },
      "outputs": [],
      "source": [
        "evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\")\n",
        "\n",
        "# We can even override the model's learned knowledge using ground truth labels\n",
        "eval_result = evaluator.evaluate_strings(\n",
        "    input=\"What is the capital of the US?\",\n",
        "    prediction=\"Bangkok\",\n",
        "    reference=\"The capital of the US is Washington D.C.\",\n",
        ")\n",
        "\n",
        "print(f'Evaluation value (Y/N): {eval_result[\"value\"]}')\n",
        "print(f'Evaluation score: {eval_result[\"score\"]}')\n",
        "print(f'Evaluation reasoning: {eval_result[\"reasoning\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXLfgyq9QQQ-"
      },
      "outputs": [],
      "source": [
        "evaluator = load_evaluator(\"labeled_criteria\", criteria=\"correctness\")\n",
        "\n",
        "# We can even override the model's learned knowledge using ground truth labels\n",
        "eval_result = evaluator.evaluate_strings(\n",
        "    input=\"What is the capital of the US?\",\n",
        "    prediction=\"Washington D.C.\",\n",
        "    reference=\"The capital of the US is Washington D.C.\",\n",
        ")\n",
        "\n",
        "print(f'Evaluation value (Y/N): {eval_result[\"value\"]}')\n",
        "print(f'Evaluation score: {eval_result[\"score\"]}')\n",
        "print(f'Evaluation reasoning: {eval_result[\"reasoning\"]}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gLRbJPBxEQLE",
        "RhvK9K9-daNz",
        "JghPkv4NW6CZ",
        "dj3kR-n4BJi6",
        "pEVhSc0knNhD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}